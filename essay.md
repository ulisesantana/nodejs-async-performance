# C칩mo pas칠 un proceso en Node.js de 5 horas a 5 minutos

Hola! Hoy vengo aqu칤 a hablar de c칩mo pas칠 un proceso en Node.js de 5 horas a 5 minutos. Sin embargo, antes me gustar칤a presentarme. Soy Ulises Santana y trabajo como Full Stack Developer en Lean Mind donde ayudamos a otras empresas a hacer que sus proyectos de software sean m치s sostenibles. Soy de Gran Canaria, pero actualmente vivo en El Hierro desde donde trabajo en remoto. Tengo una perrita llamada Mocha (en honor al nombre en clave de JavaScript cuando Brendan Eich empez칩 a trabajar en 칠l) y un gatete llamado Null. 

Antes de explicar c칩mo pas칠 un proceso en Node.js de 5 horas a 5 minutos, vamos a empezar el contexto de esta historia. Estaba trabajando para un cliente en un equipo de 5 personas, en el que las otras 4 se hab칤an incorporado en los 칰ltimos 3 meses, mientras que yo llevaba casi un a침o con el cliente. En este equipo mi rol era el de Senior Node.js Developer y era el 칰nico que ten칤a experiencia previa trabajando con Node.js. Aparte hab칤a otra persona con experiencia con JavaScript y Dart, lo cual hac칤a que le resultara f치cil adaptarse a los proyectos en TypeScript, que es el lenguaje en el que estaban todos los proyectos. Sin embargo, las otras tres personas del equipo ten칤an muy poca experiencia previa en JavaScript.

Por otro lado, est치bamos trabajando en las distintas partes de un motor de facturaci칩n que necesitaba ser adaptado para un cambio legislativo. Esto 칰ltimo significa que el deadline no se pod칤a mover, si el cambio no estaba hecho para esa fecha la empresa no pod칤a generar la facturaci칩n del siguiente mes. En caso de que no lleg치ramos le romp칤amos el cash flow. Suave, sin presi칩n.

Por concluir esta contextualizaci칩n:

- Se iba a rehacer un proyecto desde cero y hab칤an modificaciones en varios m치s.
- No todo el equipo controlaba la tecnolog칤a en la que se estaba trabajando.
- El deadline es fijo y cr칤tico, ya que rompemos el cash flow de la empresa en caso de retrasarnos.

## La calma antes de la tormenta

A Abraham Lincoln se le atribuye la siguiente frase: *Dame 6 horas para cortar un 치rbol y pasar칠 4 afilando el hacha*. Sab칤amos que iba a haber un cambio legislativo que conllevar칤an cambios en los proyectos, as칤 que desde 2 meses antes del deadline propusimos refactorizar partes de los proyectos y uno en concreto solicitamos rehacerlo desde 0, ya que en ese entonces era realmente un prototipo que funcionaba, pero costaba mantener y con el cambio legislativo se iba a hacer m치s insostenible. Vamos a llamar a este proyecto el *Proyecto Le침ador*.

![](assets/lumberjack.jpeg)

En Lean Mind por regla general trabajamos haciendo pair o mob programming, por lo que nadie nunca est치 solo y as칤 facilitamos que el c칩digo sea m치s sostenible y que tanto la autor칤a del c칩digo como el conocimiento se comparta. Sin embargo, como ten칤amos 5 proyectos que actualizar decidimos dividirnos lo m치ximo posible para poder abarcar al menos 3 proyectos a la vez y poder tener los cambios lo antes posible. Eso sale a 2 personas por proyecto y una persona sola. Esa persona que se qued칩 sola fui yo y estuve a varias bandas asistiendo a los diferentes equipos a la par que trabajaba en el proyecto en el que me tocaba. 

Esta situaci칩n hizo que por falta de tiempo descuidara el proceso de code review y simplemente me centrara en resolver las dudas del equipo sobre todo en dominio, ya que recuerdo que el resto del equipo llevaba s칩lo 3 meses con el cliente y el dominio del negocio ten칤a una curva de asimilaci칩n de al menos 6 meses. Adem치s, como hac칤amos TDD, si los test pasan y reflejaban bien las especificaciones de negocio no hab칤a de qu칠 preocuparse. 

El *Proyecto Le침ador* fue llevado a cabo por miembros del equipo que no ten칤an mucha experiencia en JavaScript y ninguna en TypeScript. Esto no supon칤a a priori ning칰n problema porque ya llevaban tres meses haciendo pair o mob programming con miembros del equipo que s칤 ten칤an experiencia y estas mismas personas hab칤an hecho aportaciones a los diferentes proyectos en TypeScript. Simplemente ped칤an ayuda o consejo cuando lo necesitaban y se les asist칤a.

La realidad es que el salto de calidad era m치s que evidente. No vi el proyecto en su estado final directamente, sino que lo vi evolucionar a lo largo de las semanas y realmente era mucho m치s claro en su prop칩sito y no hab칤a sorpresas en la implementaci칩n. Yo hab칤a sido parte del equipo que hab칤a hecho ese prototipo 9 meses atr치s y la verdad es que hab칤a ciertas partes que para m칤 eran un pel칤n oscuras, que no terminaba de entender c칩mo funcionaban o cu치l era su prop칩sito final.

Estaba muy orgulloso de lo que el equipo hab칤a conseguido, realmente era un proyecto mucho m치s sostenible, eliminando sorpresas. Sin embargo, cuando estaba terminado e hice una 칰ltima revisi칩n algo m치s extensa con el equipo ve칤a algunos flujos de datos que ten칤an toda la pinta de bloquear el Event Loop, o al menos parte de 칠l, perdiendo performance. Para comprobar el performance de este nuevo proyecto pas칠 al siguiente paso que ten칤amos planeado: hacer una prueba comparando el prototipo original con el *Proyecto Le침ador*. 

El prototipo original bas치ndose en un set de datos de unos cientos de miles de registros era capaz de hacerlo todo en unos 7 minutos. Con el mismo set de datos prob칠 con el *Proyecto Le침ador* y el resultado fue que tard칩 nada m치s y nada menos que **5 horas 7 minutos y 54 segundos.** Estamos hablando de que tardaba 44 veces m치s. El proceso real en producci칩n tardaba cada noche unos 40 minutos, por lo que si mand치bamos esto a producci칩n el nuevo proceso tardar칤a unas 29 horas y 20 minutos, esta p칠rdida de performance era inasumible. En ese momento mi yo interno era algo as칤:

![](assets/gifs/coffin-dance.gif) 

Por meter m치s le침a al fuego [쯟o pillas? Le침a, *Proyecto Le침ador* 游뱎], esto pas칩 a unos 10 d칤as del deadline, 10 d칤as naturales. No pod칤amos replantear el proyecto, hab칤a que optimizarlo en menos de una semana, adem치s de que hab칤a m치s cosas en la parrilla. En esta situaci칩n, por un lado me motivaba a m칤 mismo pensando cosas del tipo *Llevo toda mi vida prepar치ndome para este momento, los talleres sobre asincron칤a en Node.js con Matteo Collina y James Snell van a dar sus frutos* [Por cierto, un saludo desde aqu칤 a Matteo Collina y James Snell]. Sin embargo, otra parte de m칤 era una mezcla de esto:

![](assets/gifs/sheldon-bag.gif)
![](assets/gifs/pickle-rick.gif)

La realidad es que entr칠 en modo p치nico y empec칠 a refactorizar el proyecto y tratar de mejorar en performance todo lo que pod칤a. No cambi칠 nada de l칩gica, me limit칠 a cambiar el flujo de datos as칤ncrono, que era mayormente todo lo que tuviera que ver con leer o escribir en base de datos.

## Campeando la tormenta

Tras este refactor vi ciertos patrones que quiero remarcar y mostrar:

### 1. Evita async innecesarios

```js
export function randomNumber() {
    return Math.random()
}

export async function asyncRandomNumber() {
    return Math.random()
}
```

Estas dos funciones son id칠nticas a excepci칩n de que la segunda es as칤ncrona. Sin ning칰n motivo, pero es as칤ncrona. Cada vez que usamos async en una funci칩n, estamos autom치ticamente haciendo que devuelva una promesa, y eso hay que gestionarlo de m치s. 

Puede parecer una tonter칤a, pero afecta. Hice un peque침o script para demostrar hasta qu칠 punto esto afecta a nuestro performance. Lo que hace el script es ejecutar cada una de estas funciones por separado un mill칩n de veces.

(_Demo async-await/useless-async_)

Como podemos ver, s칩lo por poner ese `async` hemos hecho que tarde casi seis veces m치s.

Aplic치ndolo a la vida real, en una suite de test (bastante grande) reducimos un 40% el tiempo que tardaba en ejecutarse s칩lo quitando los async innecesarios que se nos hab칤an quedado despu칠s de un refactor.


### 2. Evita los await dentro de los bucles.

Imagina que tienes una tarea que hacer que sea en base a una lista de IDs tienes que recuperar informaci칩n de una API. Por limitaciones t칠cnicas no puedes pasarle a la API la lista de IDs, sino que tienes que hacer una llamada por cada ID. 쮺u치l es la primera idea que se nos viene a la cabeza? Probablemente un bucle, apesta a bucle. La implementaci칩n podr칤a ser algo as칤:

```js
async function fetchUserListInfo(ids) {
  const values = []
  for (const id of ids) {
    values.push(await fetchUserInfo(id))
  }
  return values
}
```

Parece sencillo y adem치s si lo probamos veremos que funciona. Consigue la informaci칩n de todos los usuarios sin problemas. 쯉in problemas? Ese await dentro del `for` fuerza a que se termine de completar cada promesa antes de procesar la siguiente. Esto significa que si de media la API tarda en responder 50ms y tenemos 100 IDS que procesar, tardaremos unos 5 segundos en realizar la tarea. No es que sea un drama, pero si implementamos esta otra soluci칩n la cosa cambia bastante:

```js
function fetchUserListInfo(ids) {
  const values = []
  for (const id of ids) {
    values.push(fetchUserInfo(id))
  }
  return Promise.all(values)
}
```

Aqu칤 vemos hay tres sutiles diferencias:
1. La funci칩n ya no es as칤ncrona, aunque s칤 que devuelve una promesa.
1. Dentro del for ya no hacemos un await
1. Devolvemos un Promise.all en vez de los valores como hac칤amos antes.

La gran diferencia de esta soluci칩n es que dentro del bucle no resolvemos las promesas, sino que simplemente las a침adimos a nuestro array pendiente de ser resueltas. Cualquier funci칩n que devuelva una promesa la devuelve en este estado que hasta que no uses `await` o `.then` no estar치 *fulfilled* o *rejected*. Puedes verlo como el experimento del gato de Schrodinger, hasta que no abres la caja no sabes si el gato est치 vivo o muerto. A las promesas les pasa algo parecido, hasta que no las resuelves est치n en estado *pending* y una vez resueltas pueden estar *fulfilled*, que es cuando se ha resuelto satisfactoriamente, o *rejected* que es cuando ha habido alg칰n error.

Volviendo a la soluci칩n, vemos que las promesas no se resuelven, sino que se almacenan directamente en estado *Pending* y la funci칩n al devolverlas, las resuelve todas *a la vez*. Esto hace que ahora la tarea se haga mucho m치s r치pido, tardando lo que tarde en responderse la llamada a la API m치s lenta. 

Voy a mostrar otra demo en la que usamos casi el mismo c칩digo, lo 칰nico que cambia es que en vez de llamar a una API, espera un milisegundo y devolver una string. Lo que vamos a ver es cuanto tarda cada una de las soluciones ejecutando esta tarea para una lista de 100 IDS y lo va a hacer 1000 veces para que podamos ver si realmente hay una diferencia de performance o no.

Como vemos la diferencia es abismal. Para la misma tarea cuando usamos `Promise.all` tarda poco m치s de 1 segundo, pero cuando usamos `await` dentro del for tarda **casi 2 minutos**.

### 3. Usa Promise.all siempre que puedas

En nuestro d칤a a d칤a desarrollando soluciones de software nos encontramos que en m치s de una ocasi칩n necesitamos varios recursos de diferentes sitios, ya sean tablas, bases de datos o APIs. Todo esto adem치s tiene una naturaleza as칤crona y necesitamos gestionarla.

```js
async function readAllUserInfo(userId) {
  const user = await readUser(userId)
  const contracts = await readContractsForUser(userId)
  const invoices = await readInvoicesForUser(userId)
  return {
    ...user,
    contracts,
    invoices
  }
}
```

Aqu칤 vemos un problema parecido al anterior, pero sin bucles. Aunque es menos dram치tico, es otro de los sitios de donde podemos rascar performance si utilizamos `Promise.all`, ya que como vemos ninguna de las peticiones depende de la otra, por lo que podr칤amos pedir toda la informaci칩n a la vez y as칤 reducir el tiempo que necesita la funci칩n para realizar la tarea.

```js
async function readAllUserInfo(userId) {
  const [ user, contracts, invoices ] = await Promise.all([
    readUser(userId),
    readContractsForUser(userId),
    readInvoicesForUser(userId)
  ])
  return {
    ...user,
    contracts,
    invoices
  }
}
```

### 4. S칠 consciente de cuantas promesas est치s gestionando

```js
function fetchUserListInfo(ids) {
  return Promise.all(ids.map(fetchUserInfo))
}
```

Esta ser칤a otra forma de hacer el `fetchUserListInfo` que hemos visto un par de diapositivas atr치s. Tanto esta como la anterior soluci칩n tienen un problema, no sabes cu치ntas promesas vas a tener en el `Promise.all`. En casos en los que no sabes el n칰mero de promesa que vas a gestionar o este n칰mero es muy alto es recomendable usar la librer칤a `p-map` y limitar la concurrencia. La raz칩n para hacer esto es que si tienes demasiadas promesas puedes acabar haci칠ndote un DDoS a ti mismo sin darte cuenta. En el *Proyecto Le침ador* m치s que un DDoS lo que nos preocupaba era ahogar la base de datos. En estos proyectos la pr치ctica habitual era limitar la concurrencia al n칰mero de conexiones que ten칤amos configurado para la base de datos, evitando as칤 ahogar la base de datos. 

```js
import pMap from 'p-map';

function fetchUserListInfo(ids) {
  return pMap(
    ids, 
    fetchUserInfo, 
    {concurrency: 10}
  )
}
```

S칩lo tener en cuenta que `p-map` pas칩 a ser de tipo modules y a menos que tu proyecto est칠 hecho de esta manera no te va a funcionar. Para poder usarlo con CommonJS necesitas tirar de la versi칩n 4. La realidad es que ambas versiones s칩lo difieren en si funcionan con ESModules o con CommonJS.

### 5. Cachear queries

Esto no es exclusivo de Node.js, pero era algo que no est치bamos haciendo y que pod칤a ayudar, ya que hab칤a ciertos datos que no cambiaban durante la ejecuci칩n y que no ven칤a mal tenerlos cacheados. 

Lo que s칤 es exclusivo de Node.js es c칩mo cachear esta clase de datos. No cacheas el valor, sino la promesa que te devuelve. Ya despu칠s cuando coges la promesa cacheada la resuelves y sigues trabajando con normalidad.

Cacheando por valor: 
```js
function CacheByValue() {
  let value = undefined

  return {
    async getNumber() {
      if (value === undefined) {
        value = await doSomethingAsync()
      }
      return value
    }
  }
}
``` 

Cacheando por promesa: 
```js
function CacheByPromise() {
  let value = undefined

  return {
    getNumber() {
      if (value === undefined) {
        value = doSomethingAsync()
      }
      return value
    }
  }
}
``` 

En esta otra demo hecho a correr estos dos trozos de c칩digo cien millones de veces para ver si realmente hay diferencia. La hay aunque es m칤nima. 

(_Demo best-practices/cached-promises_)

### 6. Haz caso de los warnings

No s칠 si a alguien m치s le pasa que la mayor칤a del tiempo ignoras los warnings y s칩lo le das importancia cuando son errores. En la consola al ejecutar el proceso me sal칤a esto:
    
```shell
(node) warning: possible EventEmitter memory leak detected. 
11 listeners added. 
Use emitter.setMaxListeners() to increase limit.
```

Yo pensaba *Meh, es un warning*. En una primera instancia simplemente hice lo que me dec칤a y aument칠 los listeners sin darle mayor importancia. Sin embargo, el proceso todav칤a era demasiado lento as칤 que investigu칠 el warning. 

El problema era que hab칤a event handlers que se estaban creando continuamente con cada conexi칩n que se solicitaba al pool de conexiones de la base de datos, pero que no se estaban eliminando, siendo el causante del memory leak. Tras implementar el fix en el que cada vez que se devuelve una conexi칩n al pool se limpian los event handlers asociados a la conexi칩n vimos una mejor칤a en la performance, tardando 4 veces menos de lo que tardaba antes.  

## Resultado  

Con todas estas mejoras en el *Proyecto Le침ador* lo volvimos a ejecutar con el mismo set de datos y en esta ocasi칩n tard칩 **04:52**. Era incluso m치s r치pido que el prototipo original que tardaba 7 minutos. En ese momento mi yo interior era algo as칤:

![](assets/gifs/fuck-yeah-dude.gif) 

Y ya por estar completamente seguros de que hab칤a una mejora, ejecutamos una prueba con un set de datos semejante al que se enfrentaba en producci칩n d칤a a d칤a, que eran millones de registros en la base de datos. En este caso el resultado qued칩 claro:

- Prototipo: 41:29 minutos  
- *Proyecto Le침ador:* 31:56 minutos

Aparte de haber mejorado la sostenibilidad del proyecto, hab칤amos mejorado la performance m치s de un 20%. Y todo esto llegando al deadline.

## Cosas que aprend칤

Tras esta experiencia saqu칠 un par de cosas en claro:

- **La asincron칤a en JS es m치s incomprendida de lo que pensaba**. La gesti칩n de asincron칤a en JavaScript es algo que no todo el mundo tiene interiorizado. Todo el mundo usa promesas y el async/await y empezar a trabajar con JavaScript o TypeScript no es complicado, lo que s칤 es m치s complicado es saber c칩mo gestionar la asincron칤a en JavaScript.
- **Forma a tu equipo**, comparte el conocimiento. Busca tiempo en la semana para poco a poco ir form치ndolo. En mi experiencia el mob programming ayuda bastante, pero no es suficiente. Algunos conceptos necesitas interiorizarlos y para eso lo mejor es hacer katas o tener formaciones con objetivos concretos. Al principio es bastante duro preparar esta clase de formaciones, pero a medida que las tengas podr치s reusarlas a medida que entren personas nuevas o si cambias de proyecto puedes formar a ese nuevo equipo.
- **No necesitas un ej칠rcito de Seniors aka personas experimentadas en X tecnolog칤a**. Con que haya un Senior que controle la tecnolog칤a es suficiente, pero la responsabilidad tecnol칩gica no debe recaer enteramente en 칠l, por lo que debe compartir el conocimiento.
- **S칠 prescindible.** Si eres prescindible no ser치s el cuello de botella. Ojo, prescindible que no innecesario. Con esto quiero decir que no seas cr칤tico por conocimiento. Trata siempre de compartir tu conocimiento con el equipo.

Y esta ha sido la historia y el aprendizaje de c칩mo pas칠 un proceso en Node.js de 5 horas a 5 minutos.

## Bonus

### 1. No mezcles tipos de asincron칤a. 

Aparte de ser m치s dif칤cil de leer, tambi칠n acaba afectando al performance porque la mayor칤a de las veces lo que hacemos es complicar el comportamiento as칤ncrono.

(Meto un ejemplo)

### 2. Async generators, ese gran desconocido

Donde brilla de mala manera es en la gesti칩n de streams. Aparte de eso, el otro caso de uso que m치s partido le he sacado es leyendo por lotes de una base de datos.

### 3. Investiga sobre el Evento Loop 

Comentar el documento de Node.js que tardaron 6 meses en escribir.